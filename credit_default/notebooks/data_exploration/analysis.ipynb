{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Loan Default Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/loan-default-prediction/train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data[\"loss\"]\n",
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_default_loss = train_data[train_data[\"loss\"]!=0]\n",
    "proportion_of_defaults = float(non_zero_default_loss.shape[0])/float(target.shape[0])\n",
    "print(r\"proportion of loans that defaulted: {ratio} %\".format(ratio= proportion_of_defaults*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_default_loss[\"loss\"].plot.hist(bins=20,by=\"loss\", log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings about target variable\n",
    "\n",
    "About 9.3% of all loans default.\n",
    "From those the most them default only a small proportion. Except for a full 100% default (which is relatively small proprotion) there seems to be a powerlaw distribution of defaulted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [column for column in train_data.columns.values if column not in [\"id\", \"loss\"]]\n",
    "train_features = train_data[feature_columns]\n",
    "\n",
    "def get_populated_columns_names(df, threshold):\n",
    "    dict_for_missing_values = dict(df.isna().any())\n",
    "    \n",
    "    columns_with_missing_values = [key for key in dict_for_missing_values if dict_for_missing_values[key]]\n",
    "    columns_without_missing_values =[key for key in dict_for_missing_values if key not in columns_with_missing_values]\n",
    "    \n",
    "    print(r\"There are {count} columns with missing values\".format(count=len(columns_with_missing_values)))\n",
    "    print(r\"There are {count} columns without missing values\".format(count=len(columns_without_missing_values)))\n",
    "\n",
    "    columns_missing_values_ratio = df.isnull().mean()\n",
    "    \n",
    "    return list(columns_missing_values_ratio[columns_missing_values_ratio < threshold].index)\n",
    "    \n",
    "train_features = train_features[get_populated_columns_names(train_features, 0.05)]\n",
    "print(r\"Number of features with less then {ratio}% missing values: {count}\".format(ratio=0.1*100, count=(train_features.shape[1]-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical/Numerical column separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_by_type(df):\n",
    "    \n",
    "    df_data_types = df.dtypes\n",
    "    numeric_var = [key for key in dict(df_data_types)\n",
    "                       if dict(df_data_types)[key]\n",
    "                           in ['float64','float32']]\n",
    "\n",
    "    int_var = [key for key in dict(df_data_types)\n",
    "                       if dict(df_data_types)[key]\n",
    "                           in ['int32','int64']]\n",
    "\n",
    "    cat_var = [key for key in dict(df_data_types)\n",
    "                 if dict(df_data_types)[key] in ['object']]\n",
    "    \n",
    "    return numeric_var, int_var, cat_var\n",
    "\n",
    "numeric_var, int_var, cat_var = get_column_names_by_type(train_features)\n",
    "\n",
    "print(r\"There are {count} float type columns\".format(count=len(numeric_var)))\n",
    "print(r\"There are {count} int type columns\".format(count=len(int_var)))\n",
    "print(r\"There are {count} object type columns\".format(count=len(cat_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns With same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_with_distinct_values(df, column_subset):\n",
    "    groups = []\n",
    "    redundant_columns = []\n",
    "    for i in range(len(column_subset)):\n",
    "        col1 = column_subset[i]\n",
    "        if col1 in redundant_columns:\n",
    "                continue\n",
    "        same_columns = [col1]\n",
    "        \n",
    "        for j in range(i, len(column_subset)):\n",
    "            col2 = column_subset[j]\n",
    "            if col1 == col2:\n",
    "                continue\n",
    "            if (df[col1]-df[col2]).sum() == 0:\n",
    "                same_columns += [col2]\n",
    "                redundant_columns += [col2]\n",
    "        groups+=[same_columns]\n",
    "    return [i[0] for i in groups]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_numeric_var = get_columns_with_distinct_values(train_features, numeric_var)\n",
    "distinct_int_var = get_columns_with_distinct_values(train_features, int_var)\n",
    "# distinct_cat_var = get_columns_with_distinct_values(train_features, cat_var)\n",
    "\n",
    "print(r\"There are {count} distinct float type columns\".format(count=len(distinct_numeric_var)))\n",
    "print(r\"There are {count} distinct int type columns\".format(count=len(distinct_int_var)))\n",
    "# print(r\"There are {count} distinct object type columns\".format(count=len(distinct_cat_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical column analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv in cat_var:\n",
    "    print(r\"{cv} has {distinct} distinct values of type {type} and sample value {value}\".format(\n",
    "        cv=cv,\n",
    "        distinct=len(train_data[cv].unique()),\n",
    "        type=type(train_data[cv][0]),\n",
    "        value = train_data[cv][0]\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness and Standard Devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caluculate_skewness_and_std(df):\n",
    "    skewnesses = df.skew(axis=0)\n",
    "    standard_devs = df.std(axis=0)\n",
    "    return skewnesses, standard_devs\n",
    "\n",
    "skewnesses, standard_devs = caluculate_skewness_and_std(train_features[distinct_numeric_var + distinct_int_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_no_standard_dev = list(standard_devs[standard_devs<1e-4].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_no_standard_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_features.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.drop(columns=columns_with_no_standard_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_int_var = [i for i in distinct_int_var if i not in columns_with_no_standard_dev]\n",
    "distinct_numeric_var = [i for i in distinct_numeric_var if i not in columns_with_no_standard_dev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropies_of_numeric_vars(df, columns):\n",
    "    tmp = [(column, entropy(np.histogram(train_features[column].dropna().values, bins=10000)[0])) for column in columns]\n",
    "    return sorted(tmp, key = lambda x: x[1])\n",
    "entropies_of_numeric = calculate_entropies_of_numeric_vars(train_features, distinct_numeric_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_of_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(entropies_of_numeric)), list(map(lambda x: x[1],entropies_of_numeric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_numeric_var_with_low_entropy = [i[0] for i in entropies_of_numeric if i[1]<4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Columns maskarading as numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_count_of_unique_integer_values(df, column_names):\n",
    "    unique_integer_values = [(iv, len(df[iv].unique())) for iv in column_names]\n",
    "    tmp = sorted(unique_integer_values, key = lambda x: x[1])\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "sorted_count_of_unique_integer_values = get_sorted_count_of_unique_integer_values(train_features, distinct_int_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_integer_type_data(df, column_names, columns, rows):\n",
    "   \n",
    "    fig, ax = plt.subplots(columns, rows, figsize=(20, 18))\n",
    "    plt.subplots_adjust(hspace = 0.4, wspace=0.4)\n",
    "    \n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for j,column_name in enumerate(column_names):\n",
    "        ax[j].hist(df[column_name[0]].values, bins=column_name[1])\n",
    "        ax[j].set_title(column_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_integer_type_data(train_features, sorted_count_of_unique_integer_values[:20], 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_converted_to_categorical = [\"f776\", \"f777\", \"f725\", \"f2\", \"f5\", \"f73\", \"f403\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropies_columns(df, column_names):\n",
    "    tmp = [(column_name, entropy(df[column_name].value_counts().values)) for column_name in column_names]\n",
    "    return sorted(tmp, key=lambda x: x[1])\n",
    "sorted_entropies = calculate_entropies_columns(train_features, distinct_int_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(sorted_entropies)), list(map(lambda x: x[1],sorted_entropies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_most_frequent_item(df, column_names):\n",
    "    tmp = [(column_name, df[column_name].value_counts().values[0]) for column_name in column_names]\n",
    "    return sorted(tmp, key=lambda x: x[1])\n",
    "most_frequent_items = calculate_most_frequent_item(train_features, distinct_int_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_int_var_with_min_frequency = [i[0] for i in most_frequent_items if i[1] > 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(most_frequent_items)), list(map(lambda x: x[1],most_frequent_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns to be deleted\n",
    "\n",
    "* All categorical columns because they have a huge cardinality\n",
    "* Data columns with more than 10% of missing values\n",
    "* Data that have 0 standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('../data/columns_to_consider', 'ab') \n",
    "pickle.dump(dict(\n",
    "    dictinct_numeric_var=distinct_numeric_var,\n",
    "    distinct_int_var=distinct_int_var,\n",
    "    distinct_int_var_with_min_frequency=distinct_int_var_with_min_frequency,\n",
    "    distinct_numeric_var_with_low_entropy=distinct_numeric_var_with_low_entropy\n",
    "    ), dbfile)                      \n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
