{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "- import the data\n",
    "- replace null values\n",
    "- separate categorical and numerical\n",
    "- convert target to binary|\n",
    "- run pca on numerical\n",
    "- one hot encoding on categorical\n",
    "- train test separation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from etl.null_value_replacer import NullValueReplacer\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/loan-default-prediction/train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_replacer = NullValueReplacer(\"median\")\n",
    "\n",
    "train_data = null_value_replacer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_types = train_data.dtypes\n",
    "cat_var = [key for key in dict(df_data_types)\n",
    "                 if dict(df_data_types)[key] in ['object']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=cat_var, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_with_distinct_values(df, column_subset):\n",
    "    groups = []\n",
    "    redundant_columns = []\n",
    "    for i in range(len(column_subset)):\n",
    "        col1 = column_subset[i]\n",
    "        if col1 in redundant_columns:\n",
    "                continue\n",
    "        same_columns = [col1]\n",
    "        \n",
    "        for j in range(i, len(column_subset)):\n",
    "            col2 = column_subset[j]\n",
    "            if col1 == col2:\n",
    "                continue\n",
    "            if (df[col1]-df[col2]).sum() == 0:\n",
    "                same_columns += [col2]\n",
    "                redundant_columns += [col2]\n",
    "        groups+=[same_columns]\n",
    "    return [i[0] for i in groups]\n",
    "\n",
    "columns_to_use = get_columns_with_distinct_values(train_data, train_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_and_split(df, ratio=0.7):\n",
    "    \n",
    "    \n",
    "    lossless_data = df[df[\"loss\"]==0]\n",
    "    lossless_data_indices = np.random.permutation(lossless_data.index.values)\n",
    "    lossless_data_split_index = math.floor(len(lossless_data_indices)*ratio)\n",
    "\n",
    "    loss_data = df[df[\"loss\"] >0 ]\n",
    "    loss_data_indices = np.random.permutation(loss_data.index.values)\n",
    "    loss_data_split_index = math.floor(len(loss_data_indices)*ratio)\n",
    "    \n",
    "    \n",
    "    test_data = pd.concat(\n",
    "            [\n",
    "                lossless_data.loc[lossless_data_indices[lossless_data_split_index:]], \n",
    "                 loss_data.loc[loss_data_indices[loss_data_split_index:]]\n",
    "            ]\n",
    "        ).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    loss_train_data = loss_data.loc[loss_data_indices[:loss_data_split_index]]\n",
    "    \n",
    "    train_data = []\n",
    "    NUMBER_OF_TRAIN_PARTITIONS = 9\n",
    "    for i in range(0, NUMBER_OF_TRAIN_PARTITIONS):\n",
    "        start_index = i * math.floor(lossless_data_split_index/NUMBER_OF_TRAIN_PARTITIONS)\n",
    "        end_index = (i + 1) * math.floor(lossless_data_split_index/NUMBER_OF_TRAIN_PARTITIONS)\n",
    "        \n",
    "        train_data += [\n",
    "            pd.concat(\n",
    "                [\n",
    "                    lossless_data.loc[lossless_data_indices[start_index: end_index]],\n",
    "                    loss_train_data\n",
    "                ]\n",
    "            ).sample(frac=1).reset_index(drop=True)\n",
    "        ]\n",
    "        \n",
    "    return train_data, test_data\n",
    "\n",
    "train_all, test_all = resample_and_split(train_data[columns_to_use])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stack_of_classifiers(list_of_df):\n",
    "\n",
    "    classifiers = []\n",
    "    for df in list_of_df:\n",
    "        X = df.drop(columns=[\"id\", \"loss\"])\n",
    "        y = df[\"loss\"].astype(\"bool\").astype(\"int\")\n",
    "        \n",
    "        random_forest_classifier = RandomForestClassifier(n_estimators=50, criterion=\"gini\", min_samples_split=10, verbose=1)\n",
    "        \n",
    "        random_forest_classifier.fit(\n",
    "            X,\n",
    "            y=y.values.reshape(-1),\n",
    "        )\n",
    "        classifiers += [random_forest_classifier]\n",
    "        \n",
    "    return classifiers\n",
    "\n",
    "trained_classifiers = train_stack_of_classifiers(train_all)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_all.drop(columns=[\"id\", \"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_all[\"loss\"].astype(\"bool\").astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [i.predict(X_test) for i in trained_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_prob = pd.DataFrame(data=predictions).agg(sum)/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(joined_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_recall= precision_recall_fscore_support(y_test, np.around(joined_prob-0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
